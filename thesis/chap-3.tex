\chapter{Literature Review}
\label{cha:3}

\section{Dataset}

Currently available datasets for article-level and media bias classification in general are quite limited, often varying in formats or covering different types of bias. The \textbf{BASIL} dataset \cite{fan-2019-basil} is the most commonly used bias dataset, containing 300 articles with both article-level and phrase-level annotations. This dataset focuses on informational bias in news articles as it appears more frequently than lexical bias. However, the obvious drawback of this dataset is the low amount of articles included as it is not nearly enough data to get a good working detection model. \textbf{NLPCSS} \cite{chen-2020-nlpcss} (6964 rows), annotated via Ad Fontes labels, contains article-level textual content with three bias labels (bias, neutral, or unknown). Focusing on political bias and unfairness, this dataset can potentially serve as a suitable resource for an article-level media bias classification task. The \textbf{BAT} \cite{spinde-2023-bat} dataset (6345 articles, 321 outlets, Ad Fontes labels) is the most suitable for article-level bias detection as it supplies both bias-score and reliability-score for the whole article. However, the dataset itself does not supply the textual content of the articles and therefore needs to be extended, something that I have been working on as well during my Master's Thesis this and last year.

Furthermore, annotating media bias is not a straightforward task. Traditionally, this is done by hiring experts and journalists to manually read and determine how biased the content is. As with \cite{spinde-2021-babe}, annotations are generally compiled and majority voted to achieve the final annotation given a particular text. It is important to use multiple annotators to minimise introducing another form of bias towards the dataset. Annotators' personal background moderately influenced their decisions and should be taken into consideration when building datasets, along with other factors such as topics, reading news habits, and honest mistakes \cite{spinde-2021-bias-words}. Clearly, this is not a cheap procedure and can be a huge bottleneck in creating a reliable media bias dataset.

Alternatively, organisations such as Allsides and Ad Fontes have their own experts and annotations which can be crawled and made use of. Several papers and datasets \cite{spinde-2023-bat,chen-2020-nlpcss,kulkarni-2018-multi-view} have utilised this approach. However, since these datasets depend on manual labeling from a third-party organisation, the selection of articles likely also introduces bias into the dataset \cite{spinde-2023-bat}.

\section{Classification}

Current State-of-the-Art in media bias classification typically employs a Transformer-based approach, either by fine-tuning or exploiting Large Language Models such as BERT \cite{devlin-2019-bert} or RoBERTa \cite{liu-2019-roberta}. \textbf{MAGPIE} \cite{horych-2024-magpie} is the first large-scale, RoBERTa-based, multi-task learning (MTL) model dedicated to bias-related tasks, a promising approach for media bias detection and can be used to enhance the accuracy and efficiency of existing models. Using the more specific model MAGPIE instead of the general language models for media bias tasks may potentially enhance performance. Unfortunately, the model is only trained for sentence-level classification and also outputs binary results.

Past works on media bias classification typically use the BASIL dataset, operating on a sentence-level and outputting binary result (either biased or not) \cite{maab-2023-lexical-bias-detection, maab-2023-target-aware, guo-2022-modeling, van-den-berg-2020-context,lee-2021-unifying,lei-2022-sentence,lei-2024-event-relation}, complemented by the lack of appropriate and adequate datasets with article-level annotations \cite{demidov-2023-political-bias-classification}.

There is limited research on article-level media bias classification and article-level text classification in general. Chen et al. \cite{chen-2020-detecting-media-bias-gaussian} utilised a Gaussian mixture model, incorporating probability distributions of frequency, positions, and sequential order of lexical and informational sentence-level bias to detect article-level bias. Pappagarai et al. \cite{pappagari-2019-hierarchical} are among the first to utilise the Transformer architecture for long sequences classification, introducing chunking and propagation methods. Kulkarni et al. \cite{kulkarni-2018-multi-view} utilised attention mechanism to model a network structure of an article to classify political ideology. Outside of media bias, Su et al. \cite{su-2021-classifying} used chunking methods combined with encoder-based '[CLS] Pooling' to extract representations on the document-level for the task of clinical document classification.

Article text length generally falls between medium to long sequences, as they are not as lengthy as legal documents or clinical studies that usually contain multiple pages of text. Most news articles stay between several hundred to several thousand words. Nevertheless, The most straightforward text classification approach of fine-tuning an encoder-based language model is not necessarily effective for article-level processing as they are mostly only able to process a maximum of 512 tokens (BERT), prompting significant information loss. Moreover, classification models that demonstrated significant results have been shown to perform poorly or even fail when they are tested on large documents \cite{wan-2019-long-length}. There exist other LLMs that are designed specifically to handle longer sequences: Longformer \cite{beltagy-2020-longformer}, BigBird \cite{zaheer-2021-bigbird}. However, these models have a different architecture than the high-performing BERT and often do not consistently surpass the baseline models in classification performance, performing only  notably better than the baseline models on only two datasets \cite{park-2022-efficient}.


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
