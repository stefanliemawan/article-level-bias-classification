\chapter{Literature Review}
\label{cha:3}

\section{Dataset}

Available datasets for article-level media bias classification are quite limited, often in different formats or covering different types of bias:
\begin{itemize}
    \item The BASIL dataset \cite{fan-2019-basil} (300 articles) is the most commonly used bias dataset out there, containing 300 articles with both article-level and phrase-level annotations. This dataset focuses on informational bias in news articles as it appears more frequently than lexical bias. However, the obvious drawback of this dataset is the low amount of articles included as it is not nearly enough data to get a good working detection model.
    \item NLPCSS \cite{chen-etal-2020-nlpcss} (6964 rows), annotated via Ad Fontes labels, contains article-level textual content with three bias labels (bias, neutral, or unknown). Focusing on political bias and unfairness, this can technically be a decent dataset for work for an article-level classification task.
    \item The BAT \cite{spinde-2023-bat} dataset (6345 rows, Ad Fontes labels) is the most suitable for article-level bias detection as it supplies both bias-score and reliability-score for the whole article. However, the dataset itself does not supply textual content of the articles and therefore need to be extended, something that I have been working on as well during my Master's Thesis this and last year.
\end{itemize}

Annotating bias is not a straightforward task. Traditionally, this is done by hiring experts and journalists to manually read and determine how biased the content is. As with \cite{spinde-2021-babe}, annotations are generally compiled and majority voted to achieve the final annotation given a particular text. It is important to use multiple annotators to minimise introducing another form of bias towards the dataset. Annotators' personal background moderately influenced their decisions and should be taken into consideration when building datasets, along with other factors such as topics, reading news habits, and honest mistakes \cite{spinde-2021-bias-words}. Clearly, this is not a cheap procedure and can be a huge bottleneck in creating a reliable media bias dataset.

Alternatively, websites such as Allsides and AdFontes have their own experts and annotations to which can be crawled and make use of (cite cite). However, they operate solely within the U.S., coverting U.S. media sources. To my knowledge, there is no other organisation that do what they do outside the U.S. or in a more global scale, at the same level. Therefore, building a dataset that is not only accurate, but also global and diverse would have massive benefits.

\begin{comment}
News Unfold: (NewsUnravel shows that a user-centric approach to media bias data collection can re- turn reliable data while being scalable and evaluated as easy to use. NewsUnravel demonstrates that feedback mechanisms are a promising strategy to reduce data collection expenses and continuously update datasets to changes in context.) (Our approach augments dataset quality by significantly increasing inter-annotator agreement by 26.31\% and improving classifier performance by 2.49\%)
\end{comment}

\section{Classification}

Current State-of-the-Art in media bias detection typically to employ neural or transformer-based approach, by fine-tuning or exploiting Large Language Models (cite).

MAGPIE \cite{horych-2024-magpie} is the first large-scale, RoBERTa-based, multi-task learning (MTL) model dedicated for bias-related task, a promising approach for media bias detection and can be used to enhance the accuracy and efficiency of existing models. Using MAGPIE's context representation instead of BERT for media detection can potentially improve performance. However, currently the model only trained for sentence-level classification and also outputs binary result.

Many past works on automatic detection of media bias (cite cite) also typically operate on a sentence level, complemented by the lack of appropriate and adequate datasets with article-level annotations \cite{demidov-2023-political-bias-classification}. They also tend to only output binary result (either biased or not) \cite{van-den-berg-2020-context, maab-2023-target-aware, lei-2024-sentence}. One of the main problem with detecting on sentence-level is that multiple sentences within the same article could have opposite or different biases (I think there is a cite for this, I remember reading).

As media content are often delivered in the form of articles containing a number of paragraphs, detection at the article level is far more useful and desirable. More advanced methods could also include global-level spanning over multiple articles by implementing timeline, graphs, or other types of network to capture relationships among articles with same or similar topics.

Using low-level lexical information alone is inadequate for identifying bias within articles, particularly when dealing with a limited dataset \cite{chen-2020-detecting-media-bias-gaussian}.

Complex models for long document classification has been shown to not consistently surpass simplistic baseline models in performance \cite{park-2022-efficient}, although this could be highly-dependant on the dataset used.

Furthermore, The traditional approach of assigning only a single frame label to news articles remains overly simplistic, given that a standard news story often incorporates multiple viewpoints, arguments, or facets, each potentially carrying distinct connotations or framing \cite{vallejo-2023-connecting}. An integer label would be a slightly better solution to represent bias from a text, although it is still hardly ideal.

Additionally, it would be good to consider explainability of a model when classifying a bias, as we would need to not just get the result, but to understand why contents are considered bias. Existing media bias detection systems typically concentrate solely on predicting the likelihood of a certain text being biased, offering limited insights into the underlying reason behind the decision.

\section{Working with article-length text}

Article texts length generally falls between medium to long sequences, as they are not as lengthy as legal documents or clinical studies that usually contains pages of text. Most news articles stay between several hundreds to several thousands words (find cite?).

Classification models demonstrate significant results might perform poorly or even fail when they are tested on large documents \cite{wan-2019-long-length}

The most straightforward approach of standard fine-tuning of an encoder-based langauge model is not necessarily effective as they mostly only able to process a maximum of 512 tokens (BERT), prompting significant information loss. Several techniques have been attempted to address this issue such as the sliding window techniques, CLS techniques \cite{su-2021-classifying}, where you exploit the 'CLS' token from a BERT embedding, and other more sophisticated models \cite{kulkarni-2018-multi-view}.

Additionally, there are other LLMs similar to BERT that are designed to handle longer sequences: Longformer (cite), BigBird (cite). However, most of these models have different architecture than the original BERT and therefore perform differently.
Advanced models such as Longformer, ToBERT, and CogLTX do not consistently surpass the baseline models in classification performance, and only performed notably better than the baseline models on only two datasets \cite{park-2022-efficient}.

% \section{Conclusion}
% The final section of the chapter gives an overview of the important results
% of this chapter. This implies that the introductory chapter and the
% concluding chapter don't need a conclusion.

\lipsum[66]

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
