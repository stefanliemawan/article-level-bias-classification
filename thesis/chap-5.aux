\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {5}Methodology}{19}{chapter.5}\protected@file@percent }
\newlabel{cha:5}{{\M@TitleReference {5}{Methodology}}{19}{Methodology}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Dataset split}{19}{section.5.1}\protected@file@percent }
\citation{devlin-2019-bert}
\citation{devlin-2019-bert}
\citation{devlin-2019-bert}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Features and baselines}{20}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Proposed methods}{20}{section.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}BoW + MLP}{20}{subsection.5.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces BoW + MLP architecture. The input article is encoded as a bag of words, then feed into a single linear multilayer perceptron before softmax operation.}}{20}{figure.5.1}\protected@file@percent }
\newlabel{fig:bow_mlp_architecture}{{\M@TitleReference {5.1}{BoW + MLP architecture. The input article is encoded as a bag of words, then feed into a single linear multilayer perceptron before softmax operation.}}{20}{BoW + MLP architecture. The input article is encoded as a bag of words, then feed into a single linear multilayer perceptron before softmax operation}{figure.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}BERT fine-tuning}{20}{subsection.5.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces BERT pre-training and fine-tuning procedures \cite  {devlin-2019-bert}}}{21}{figure.5.2}\protected@file@percent }
\newlabel{fig:bert_finetuning}{{\M@TitleReference {5.2}{BERT pre-training and fine-tuning procedures \cite  {devlin-2019-bert}}}{21}{BERT pre-training and fine-tuning procedures \cite {devlin-2019-bert}}{figure.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}BERT sliding window fine-tuning}{21}{subsection.5.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces BERT Sliding window fine-tuning architecture. The input article is split into chunks, each chunk is processed by the model as a mini batch, and the resulting logits are pooled before applying the loss function.}}{21}{figure.5.3}\protected@file@percent }
\newlabel{fig:sliding_window_architecture}{{\M@TitleReference {5.3}{BERT Sliding window fine-tuning architecture. The input article is split into chunks, each chunk is processed by the model as a mini batch, and the resulting logits are pooled before applying the loss function.}}{21}{BERT Sliding window fine-tuning architecture. The input article is split into chunks, each chunk is processed by the model as a mini batch, and the resulting logits are pooled before applying the loss function}{figure.5.3}{}}
\citation{sun-2020-fine-tune}
\citation{su-2021-classifying}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.4}CLS method}{22}{subsection.5.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces CLS method architecture}}{22}{figure.5.4}\protected@file@percent }
\newlabel{fig:cls_method}{{\M@TitleReference {5.4}{CLS method architecture}}{22}{CLS method architecture}{figure.5.4}{}}
\citation{devlin-2019-bert}
\citation{horych-2024-magpie}
\citation{agarap-2018-relu}
\citation{van-1995-python}
\citation{paszke-2017-pytorch}
\citation{wolf-2020-huggingface}
\citation{devlin-2019-bert}
\citation{loshchilov-2019-adamw}
\citation{pedregosa-2011-scikit-learn}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Training details}{23}{section.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Weighted loss}{23}{subsection.5.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Class weight in the training set}}{24}{figure.5.5}\protected@file@percent }
\newlabel{fig:class_weight}{{\M@TitleReference {5.5}{Class weight in the training set}}{24}{Class weight in the training set}{figure.5.5}{}}
\@setckpt{chap-5}{
\setcounter{page}{25}
\setcounter{equation}{0}
\setcounter{enumi}{4}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{@memmarkcntra}{0}
\setcounter{storedpagenumber}{1}
\setcounter{book}{0}
\setcounter{part}{0}
\setcounter{chapter}{5}
\setcounter{section}{4}
\setcounter{subsection}{1}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{vslineno}{0}
\setcounter{poemline}{0}
\setcounter{modulo@vs}{0}
\setcounter{memfvsline}{0}
\setcounter{verse}{0}
\setcounter{chrsinstr}{0}
\setcounter{poem}{0}
\setcounter{newflo@tctr}{4}
\setcounter{@contsubnum}{0}
\setcounter{maxsecnumdepth}{2}
\setcounter{sidefootnote}{0}
\setcounter{pagenote}{0}
\setcounter{pagenoteshadow}{0}
\setcounter{memfbvline}{0}
\setcounter{bvlinectr}{0}
\setcounter{cp@cntr}{0}
\setcounter{ism@mctr}{0}
\setcounter{xsm@mctr}{0}
\setcounter{csm@mctr}{0}
\setcounter{ksm@mctr}{0}
\setcounter{xksm@mctr}{0}
\setcounter{cksm@mctr}{0}
\setcounter{msm@mctr}{0}
\setcounter{xmsm@mctr}{0}
\setcounter{cmsm@mctr}{0}
\setcounter{bsm@mctr}{0}
\setcounter{workm@mctr}{0}
\setcounter{sheetsequence}{29}
\setcounter{lastsheet}{45}
\setcounter{lastpage}{41}
\setcounter{figure}{5}
\setcounter{lofdepth}{1}
\setcounter{table}{0}
\setcounter{lotdepth}{1}
\setcounter{section@level}{2}
\setcounter{Item}{8}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{21}
\setcounter{memhycontfloat}{0}
\setcounter{Hpagenote}{0}
\setcounter{parentequation}{0}
}
