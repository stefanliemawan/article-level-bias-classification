\documentclass[master=mai,masteroption=slt]{kulemt}
\setup{% Remove the "%" on the next line when using UTF-8 character encoding
  inputenc=utf8,
  title={Article-Level Media Bias Classification},
  author={Stefan Liemawan Adji},
  promotor={Dr. Miryam de Lhoneux},
  assessor={},
  assistant={Dr. Timo Spinde and Tomáš Horych of Media Bias Group}
  }
% Remove the "%" on the next line for generating the cover page
%\setup{coverpageonly}
% Remove the "%" before the next "\setup" to generate only the first pages
% (e.g., if you are a Word user).
%\setup{frontpagesonly}

% Choose the main text font (e.g., Latin Modern)
\setup{font=lm}

% If you want to include other LaTeX packages, do it here. 

% Finally the hyperref package is used for pdf files.
% This can be commented out for printed versions.

\usepackage{hyperref}
\hypersetup{
    pdfusetitle,
    colorlinks,
    plainpages=false,
    citecolor=magenta,  
    linkcolor=black,  
    urlcolor=black,   
    filecolor=black   
}

\usepackage{verbatim}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{multirow}
% \usepackage{algorithm}
\usepackage[linesnumbered, ruled, vlined]{algorithm2e}
\usepackage{algpseudocode}
\usepackage{longtable}
\usepackage{subcaption}


%\includeonly{chap-n}
\begin{document}

\begin{preface}
    I would like to thank my supervisor, Dr. Miryam de Lhoneux, for agreeing to oversee this project. Her support, guidance, and impeccable ability to respond to emails quickly have been a wonderful blessing. I would also like to thank Dr. Timo Spinde and The Media Bias Group for introducing the topic and allowing me to be a part of the network.
\end{preface}

\tableofcontents*

\begin{abstract}
    In today's digital, information-rich society, media bias presents a significant challenge to the objectivity and credibility of news reporting. Media bias can shape perceptions, influence opinions, and affect understanding of various issues. Recognising and addressing this bias is crucial for ensuring a well-informed and balanced perspective. Existing research on automatic media bias detection largely focuses on sentence-level analysis and produces binary outputs, thus this thesis aims to develop a system for detecting and classifying media bias at the article level with multi-class output. The BAT dataset \cite{spinde-2023-bat} was reconstructed by crawling article content and introducing four class labels derived from reliability scores that measure the truthfulness of the articles. By leveraging State-of-the-Art (SOTA) techniques and successful document classification methods, transformer-based approaches and hierarchical models are trained and evaluated using the reconstructed BAT dataset. The performances of these methods are compared under two different feature sets: one using only the articles' titles and content, and another incorporating outlet metadata as an additional feature. The results indicate that hierarchical models achieve the best performance, with fine-tuning Longformer \cite{beltagy-2020-longformer} provides comparable results. Additionally, mean-pooling with smaller chunk sizes was found to outperform CLS-pooling. Including outlet metadata as an additional feature generally improved performance across most fine-tuning methods.
\end{abstract}

% A list of figures and tables is optional
\listoffigures
\listoftables
% If you only have a few figures and tables you can use the following instead
% \listoffiguresandtables
% The list of symbols is also optional.
% This list must be created manually, e.g., as follows:
\chapter{List of Abbreviations}
\section*{Abbreviations}
\begin{flushleft}
    \renewcommand{\arraystretch}{1.1}
    \begin{tabularx}{\textwidth}{@{}p{30mm}X@{}}
        AI         & Artificial Intelligence                                                                     \\
        ANN        & Artificial Neural Networks                                                                  \\
        BAT        & Bias And Twitter                                                                            \\
        BERT       & Bidirectional Encoder Representations from Transformers                                     \\
        Bi-LSTM    & Bidirectional Long Short-Term Memory                                                        \\
        CNN        & Convolutional Neural Network                                                                \\
        DA-RoBERTa & Domain-Adaptive-RoBERTa                                                                     \\
        DocBERT    & Document BERT (BERT for Document Classification)                                            \\
        FT         & Fine-tuning                                                                                 \\
        GloVE      & Global Vectors for Word Representation                                                      \\
        GPT        & Generative Pre-trained Transformer                                                          \\
        LLM        & Large Language Model                                                                        \\
        Longformer & The Long-Document Transformer                                                               \\
        LSTM       & Long Short-Term Memory (LSTM)                                                               \\
        MAGPIE     & Multi-Task Media-Bias Analysis Generalization for Pre-Trained Identification of Expressions \\
        MLP        & MultiLayer Perceptron                                                                       \\
        MBFC       & Media Bias/Fact Check                                                                       \\
        NER        & Named Entity Recognition                                                                    \\
        NLP        & Natural Language Processing                                                                 \\
        ReLU       & Rectified Linear Unit                                                                       \\
        RoBERTa    & Robustly Optimized BERT Approach                                                            \\
        SOTA       & State-of-The-Art                                                                            \\
        TF         & Term Frequency                                                                              \\
        TF-IDF     & Term Frequency-Inverse Document Frequency                                                   \\
        IDF        & Inverse Document Frequency                                                                  \\
    \end{tabularx}
\end{flushleft}
% \section*{Symbols}
% \begin{flushleft}
%     \renewcommand{\arraystretch}{1.1}
%     \begin{tabularx}{\textwidth}{@{}p{12mm}X@{}}
%         42    & ``The Answer to the Ultimate Question of Life, the Universe,
%         and Everything'' according to                                        \\
%         $c$   & Speed of light                                               \\
%         $E$   & Energy                                                       \\
%         $m$   & Mass                                                         \\
%         $\pi$ & The number pi                                                \\
%     \end{tabularx}
% \end{flushleft}

% Now comes the main text
\mainmatter

\include{chap-1}
\include{chap-2}
\include{chap-3}
\include{chap-4}
\include{chap-5}
\include{chap-6}
\include{chap-7}

% If you have appendices:
\appendixpage*          % if wanted
\appendix
\include{app-A}
% ... and so on until
% \include{app-n}

\backmatter
% The bibliography comes after the appendices.
% You can replace the standard "abbrv" bibliography style by another one.
\bibliographystyle{abbrv}
\bibliography{references}

\end{document}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
