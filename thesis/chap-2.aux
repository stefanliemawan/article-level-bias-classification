\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{allsides-2022-bias-definition}
\citation{white-1950-case-study-selection-news}
\citation{aires-2020-information}
\citation{patterson-donsbach-1996-news-decisions}
\citation{economist-2024-incels}
\citation{economist-2024-incels}
\citation{economist-2024-incels}
\citation{spinde-2024-taxonomy}
\citation{spinde-2024-taxonomy}
\citation{spinde-2024-taxonomy}
\citation{spinde-2024-taxonomy}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {2}Background Information and Literature Review}{5}{chapter.2}\protected@file@percent }
\newlabel{cha:2}{{\M@TitleReference {2}{Background Information and Literature Review}}{5}{Background Information and Literature Review}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Media Bias}{5}{section.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A recent article from The Economist \cite  {economist-2024-incels} portraying gender discrimination with a negative-framing headline}}{6}{figure.2.1}\protected@file@percent }
\newlabel{fig:the-economist-biased-article}{{\M@TitleReference {2.1}{A recent article from The Economist \cite  {economist-2024-incels} portraying gender discrimination with a negative-framing headline}}{6}{A recent article from The Economist \cite {economist-2024-incels} portraying gender discrimination with a negative-framing headline}{figure.2.1}{}}
\citation{allcott-2017-socialmedia-2016election}
\citation{panagopoulos-2020}
\citation{rafail-2018-tea-party}
\citation{pew-2021-partisan-divides}
\citation{gallup-knight-2020-american-views}
\citation{reuters-2023-digital-news-report}
\citation{gallup-knight-2020-american-views}
\citation{reuters-2023-digital-news-report}
\citation{reuters-2023-trust}
\citation{lim-2018-understanding}
\citation{spinde-2024-taxonomy}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Media bias types as defined in the Media Bias Taxonomy \cite  {spinde-2024-taxonomy}}}{7}{figure.2.2}\protected@file@percent }
\newlabel{fig:bias-types-taxonomy}{{\M@TitleReference {2.2}{Media bias types as defined in the Media Bias Taxonomy \cite  {spinde-2024-taxonomy}}}{7}{Media bias types as defined in the Media Bias Taxonomy \cite {spinde-2024-taxonomy}}{figure.2.2}{}}
\citation{reuters-2023-false-info}
\citation{reuters-2023-false-info}
\citation{yougov-2023-frequency}
\citation{yougov-2023-frequency}
\citation{reuters-2021-digital-news-report}
\citation{allsides-2022-bias-definition}
\citation{boudana-2011-journalistic-objectivity}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces The rate of news consumers witnessing false or misleading information on several recent key topics in February 2023 \cite  {reuters-2023-false-info}}}{8}{figure.2.3}\protected@file@percent }
\newlabel{fig:consumers-witnessing-false-information-on-certain-topics-worldwide-2023}{{\M@TitleReference {2.3}{The rate of news consumers witnessing false or misleading information on several recent key topics in February 2023 \cite  {reuters-2023-false-info}}}{8}{The rate of news consumers witnessing false or misleading information on several recent key topics in February 2023 \cite {reuters-2023-false-info}}{figure.2.3}{}}
\citation{spinde-2024-taxonomy}
\citation{narayanan-2019-word-embeddings}
\citation{narayanan-2019-word-embeddings}
\citation{mikolov-2013-embeddings}
\citation{perez-enciso-2019-guide}
\citation{perez-enciso-2019-guide}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Frequency of seeing false or misleading information online in the United States as of April 2023, by age group \cite  {yougov-2023-frequency}}}{9}{figure.2.4}\protected@file@percent }
\newlabel{fig:frequency-of-seeing-false-information-online-in-the-us-2023-by-age-group}{{\M@TitleReference {2.4}{Frequency of seeing false or misleading information online in the United States as of April 2023, by age group \cite  {yougov-2023-frequency}}}{9}{Frequency of seeing false or misleading information online in the United States as of April 2023, by age group \cite {yougov-2023-frequency}}{figure.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Word Embeddings}{9}{section.2.2}\protected@file@percent }
\citation{uzair-2020-hidden-layers}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Word embeddings visualisation \cite  {narayanan-2019-word-embeddings}}}{10}{figure.2.5}\protected@file@percent }
\newlabel{fig:word-embeddings}{{\M@TitleReference {2.5}{Word embeddings visualisation \cite  {narayanan-2019-word-embeddings}}}{10}{Word embeddings visualisation \cite {narayanan-2019-word-embeddings}}{figure.2.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}MultiLayer Perceptron (MLP)}{10}{section.2.3}\protected@file@percent }
\newlabel{eq:z}{{\M@TitleReference {2.1}{MultiLayer Perceptron (MLP)}}{10}{MultiLayer Perceptron (MLP)}{equation.2.3.1}{}}
\newlabel{eq:h}{{\M@TitleReference {2.2}{MultiLayer Perceptron (MLP)}}{10}{MultiLayer Perceptron (MLP)}{equation.2.3.2}{}}
\citation{rahman-2023-lstm-networks}
\citation{rahman-2023-lstm-networks}
\citation{hochreiter-1997-lstm}
\citation{vaswani-2023-attention}
\citation{vaswani-2023-attention}
\citation{vaswani-2023-attention}
\citation{vaswani-2023-attention}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces MultiLayer Perceptron visualisation \cite  {perez-enciso-2019-guide}}}{11}{figure.2.6}\protected@file@percent }
\newlabel{fig:mlp}{{\M@TitleReference {2.6}{MultiLayer Perceptron visualisation \cite  {perez-enciso-2019-guide}}}{11}{MultiLayer Perceptron visualisation \cite {perez-enciso-2019-guide}}{figure.2.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Long Short-Term Memory (LSTM)}{11}{section.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Transformer and Large Language Model (LLM)}{11}{section.2.5}\protected@file@percent }
\citation{devlin-2019-bert}
\citation{openai-2024-gpt4}
\citation{semianalysis-gpt4}
\citation{oracle-finetuning-blog}
\citation{devlin-2019-bert}
\citation{devlin-2019-bert}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces LSTM architecture \cite  {rahman-2023-lstm-networks}}}{12}{figure.2.7}\protected@file@percent }
\newlabel{fig:lstm}{{\M@TitleReference {2.7}{LSTM architecture \cite  {rahman-2023-lstm-networks}}}{12}{LSTM architecture \cite {rahman-2023-lstm-networks}}{figure.2.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Transformer visualisation \cite  {vaswani-2023-attention}}}{13}{figure.2.8}\protected@file@percent }
\newlabel{fig:transformer}{{\M@TitleReference {2.8}{Transformer visualisation \cite  {vaswani-2023-attention}}}{13}{Transformer visualisation \cite {vaswani-2023-attention}}{figure.2.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces BERT pre-training and fine-tuning procedures \cite  {devlin-2019-bert}}}{13}{figure.2.9}\protected@file@percent }
\newlabel{fig:bert_finetuning}{{\M@TitleReference {2.9}{BERT pre-training and fine-tuning procedures \cite  {devlin-2019-bert}}}{13}{BERT pre-training and fine-tuning procedures \cite {devlin-2019-bert}}{figure.2.9}{}}
\citation{lin-2022-survey-transformers}
\citation{makridakis-2017-ai-revolution}
\citation{wu-2021-hi-transformer}
\citation{wu-2021-hi-transformer}
\citation{wu-2021-hi-transformer}
\citation{borko-1963-auto-doc-classification}
\citation{luhn-1958-business-intelligence-system}
\newlabel{eq:attention}{{\M@TitleReference {2.3}{Transformer and Large Language Model (LLM)}}{14}{Transformer and Large Language Model (LLM)}{equation.2.5.3}{}}
\newlabel{eq:multihead}{{\M@TitleReference {2.4}{Transformer and Large Language Model (LLM)}}{14}{Transformer and Large Language Model (LLM)}{equation.2.5.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Chunking and Hierarchical Techniques}{14}{section.2.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Illustration of chunking}}{14}{figure.2.10}\protected@file@percent }
\newlabel{fig:chunking}{{\M@TitleReference {2.10}{Illustration of chunking}}{14}{Illustration of chunking}{figure.2.10}{}}
\citation{box-2023-untapped}
\citation{mishra-2017-structured-unstructured}
\citation{mali-2021-relevance-of-preprocessing}
\citation{afzal-deepdocclassifier}
\citation{liu-2017-xmlcnn}
\citation{yang-2016-han}
\citation{adhikari-2019-docbert}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces An example of hierarchical transformer model, Hi-Transformer\cite  {wu-2021-hi-transformer}, to model long document}}{15}{figure.2.11}\protected@file@percent }
\newlabel{fig:hi_transformer}{{\M@TitleReference {2.11}{An example of hierarchical transformer model, Hi-Transformer\cite  {wu-2021-hi-transformer}, to model long document}}{15}{An example of hierarchical transformer model, Hi-Transformer\cite {wu-2021-hi-transformer}, to model long document}{figure.2.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Document Classification}{15}{section.2.7}\protected@file@percent }
\citation{wan-2019-long-length}
\citation{beltagy-2020-longformer}
\citation{zaheer-2021-bigbird}
\citation{park-2022-efficient}
\citation{pappagari-2019-hierarchical}
\citation{su-2021-classifying}
\citation{adhikari-2019-docbert}
\citation{khandve-2022-hierarchical-longdoc}
\citation{fan-2019-basil}
\citation{chen-2020-nlpcss}
\citation{spinde-2023-bat}
\@writefile{toc}{\contentsline {section}{\numberline {2.8}Article-Level Media Bias Classification}{16}{section.2.8}\protected@file@percent }
\citation{lim-2018-understanding}
\citation{spinde-2021-babe}
\citation{spinde-2021-bias-words}
\citation{allsides}
\citation{adfontes}
\citation{spinde-2023-bat}
\citation{chen-2020-nlpcss}
\citation{kulkarni-2018-multi-view}
\citation{spinde-2023-bat}
\citation{devlin-2019-bert}
\citation{liu-2019-roberta}
\citation{horych-2024-magpie}
\citation{maab-2023-lexical-bias-detection}
\citation{maab-2023-target-aware}
\citation{guo-2022-modeling}
\citation{van-den-berg-2020-context}
\citation{lee-2021-unifying}
\citation{lei-2022-sentence}
\citation{lei-2024-event-relation}
\citation{demidov-2023-political-bias-classification}
\citation{chen-2020-detecting-media-bias-gaussian}
\citation{kulkarni-2018-multi-view}
\citation{barbera-2021-article-classification}
\@setckpt{chap-2}{
\setcounter{page}{19}
\setcounter{equation}{4}
\setcounter{enumi}{5}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{@memmarkcntra}{0}
\setcounter{storedpagenumber}{1}
\setcounter{book}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{8}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{vslineno}{0}
\setcounter{poemline}{0}
\setcounter{modulo@vs}{0}
\setcounter{memfvsline}{0}
\setcounter{verse}{0}
\setcounter{chrsinstr}{0}
\setcounter{poem}{0}
\setcounter{newflo@tctr}{4}
\setcounter{@contsubnum}{0}
\setcounter{maxsecnumdepth}{2}
\setcounter{sidefootnote}{0}
\setcounter{pagenote}{0}
\setcounter{pagenoteshadow}{0}
\setcounter{memfbvline}{0}
\setcounter{bvlinectr}{0}
\setcounter{cp@cntr}{0}
\setcounter{ism@mctr}{0}
\setcounter{xsm@mctr}{0}
\setcounter{csm@mctr}{0}
\setcounter{ksm@mctr}{0}
\setcounter{xksm@mctr}{0}
\setcounter{cksm@mctr}{0}
\setcounter{msm@mctr}{0}
\setcounter{xmsm@mctr}{0}
\setcounter{cmsm@mctr}{0}
\setcounter{bsm@mctr}{0}
\setcounter{workm@mctr}{0}
\setcounter{sheetsequence}{23}
\setcounter{lastsheet}{60}
\setcounter{lastpage}{56}
\setcounter{figure}{11}
\setcounter{lofdepth}{1}
\setcounter{table}{0}
\setcounter{lotdepth}{1}
\setcounter{section@level}{1}
\setcounter{Item}{5}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{14}
\setcounter{memhycontfloat}{0}
\setcounter{Hpagenote}{0}
\setcounter{parentequation}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{0}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
}
