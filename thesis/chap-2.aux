\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{allsides-2022-bias-definition}
\citation{white-1950-case-study-selection-news}
\citation{patterson-donsbach-1996-news-decisions}
\citation{spinde-2024-taxonomy}
\citation{spinde-2024-taxonomy}
\citation{spinde-2024-taxonomy}
\citation{spinde-2024-taxonomy}
\citation{pew-2021-partisan-divides}
\citation{gallup-knight-2020-american-views}
\citation{reuters-2023-digital-news-report}
\citation{gallup-knight-2020-american-views}
\citation{reuters-2023-digital-news-report}
\citation{reuters-2023-trust}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {2}Background Information and Literature Review}{5}{chapter.2}\protected@file@percent }
\newlabel{cha:2}{{\M@TitleReference {2}{Background Information and Literature Review}}{5}{Background Information and Literature Review}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Media Bias}{5}{section.2.1}\protected@file@percent }
\citation{lim-2018-understanding}
\citation{spinde-2024-taxonomy}
\citation{reuters-2023-false-info}
\citation{reuters-2023-false-info}
\citation{yougov-2023-frequency}
\citation{yougov-2023-frequency}
\citation{narayanan-2019-word-embeddings}
\citation{narayanan-2019-word-embeddings}
\citation{mikolov-2013-embeddings}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Media bias types as defined in the Media Bias Taxonomy \cite  {spinde-2024-taxonomy}}}{6}{figure.2.1}\protected@file@percent }
\newlabel{fig:bias-types-taxonomy}{{\M@TitleReference {2.1}{Media bias types as defined in the Media Bias Taxonomy \cite  {spinde-2024-taxonomy}}}{6}{Media bias types as defined in the Media Bias Taxonomy \cite {spinde-2024-taxonomy}}{figure.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Word Embeddings}{6}{section.2.2}\protected@file@percent }
\citation{perez-enciso-2019-guide}
\citation{perez-enciso-2019-guide}
\citation{uzair-2020-hidden-layers}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces The rate of news consumers witnessing false or misleading information on several recent key topics in February 2023 \cite  {reuters-2023-false-info}}}{7}{figure.2.2}\protected@file@percent }
\newlabel{fig:consumers-witnessing-false-information-on-certain-topics-worldwide-2023}{{\M@TitleReference {2.2}{The rate of news consumers witnessing false or misleading information on several recent key topics in February 2023 \cite  {reuters-2023-false-info}}}{7}{The rate of news consumers witnessing false or misleading information on several recent key topics in February 2023 \cite {reuters-2023-false-info}}{figure.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}MultiLayer Perceptron (MLP)}{7}{section.2.3}\protected@file@percent }
\newlabel{eq:z}{{\M@TitleReference {2.1}{MultiLayer Perceptron (MLP)}}{7}{MultiLayer Perceptron (MLP)}{equation.2.3.1}{}}
\newlabel{eq:h}{{\M@TitleReference {2.2}{MultiLayer Perceptron (MLP)}}{7}{MultiLayer Perceptron (MLP)}{equation.2.3.2}{}}
\citation{rahman-2023-lstm-networks}
\citation{rahman-2023-lstm-networks}
\citation{hochreiter-1997-lstm}
\citation{vaswani-2023-attention}
\citation{vaswani-2023-attention}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Frequency of seeing false or misleading information online in the United States as of April 2023, by age group \cite  {yougov-2023-frequency}}}{8}{figure.2.3}\protected@file@percent }
\newlabel{fig:frequency-of-seeing-false-information-online-in-the-us-2023-by-age-group}{{\M@TitleReference {2.3}{Frequency of seeing false or misleading information online in the United States as of April 2023, by age group \cite  {yougov-2023-frequency}}}{8}{Frequency of seeing false or misleading information online in the United States as of April 2023, by age group \cite {yougov-2023-frequency}}{figure.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Long Short-Term Memory (LSTM)}{8}{section.2.4}\protected@file@percent }
\citation{vaswani-2023-attention}
\citation{vaswani-2023-attention}
\citation{devlin-2019-bert}
\citation{openai-2024-gpt4}
\citation{semianalysis-gpt4}
\citation{oracle-finetuning-blog}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Word embeddings visualisation \cite  {narayanan-2019-word-embeddings}}}{9}{figure.2.4}\protected@file@percent }
\newlabel{fig:word-embeddings}{{\M@TitleReference {2.4}{Word embeddings visualisation \cite  {narayanan-2019-word-embeddings}}}{9}{Word embeddings visualisation \cite {narayanan-2019-word-embeddings}}{figure.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Transformer and Large Language Model (LLM)}{9}{section.2.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces MultiLayer Perceptron visualisation \cite  {perez-enciso-2019-guide}}}{10}{figure.2.5}\protected@file@percent }
\newlabel{fig:mlp}{{\M@TitleReference {2.5}{MultiLayer Perceptron visualisation \cite  {perez-enciso-2019-guide}}}{10}{MultiLayer Perceptron visualisation \cite {perez-enciso-2019-guide}}{figure.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces LSTM architecture \cite  {rahman-2023-lstm-networks}}}{10}{figure.2.6}\protected@file@percent }
\newlabel{fig:lstm}{{\M@TitleReference {2.6}{LSTM architecture \cite  {rahman-2023-lstm-networks}}}{10}{LSTM architecture \cite {rahman-2023-lstm-networks}}{figure.2.6}{}}
\citation{devlin-2019-bert}
\citation{devlin-2019-bert}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Transformer visualisation \cite  {vaswani-2023-attention}}}{11}{figure.2.7}\protected@file@percent }
\newlabel{fig:transformer}{{\M@TitleReference {2.7}{Transformer visualisation \cite  {vaswani-2023-attention}}}{11}{Transformer visualisation \cite {vaswani-2023-attention}}{figure.2.7}{}}
\newlabel{eq:attention}{{\M@TitleReference {2.3}{Transformer and Large Language Model (LLM)}}{11}{Transformer and Large Language Model (LLM)}{equation.2.5.3}{}}
\newlabel{eq:multihead}{{\M@TitleReference {2.4}{Transformer and Large Language Model (LLM)}}{11}{Transformer and Large Language Model (LLM)}{equation.2.5.4}{}}
\citation{lin-2022-survey-transformers}
\citation{makridakis-2017-ai-revolution}
\citation{horych-2024-magpie}
\citation{horych-2024-magpie}
\citation{horych-2024-magpie}
\citation{aghajanyan-2021-muppet}
\citation{horych-2024-magpie}
\citation{sun-2020-fine-tune}
\citation{wu-2021-hi-transformer}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces BERT pre-training and fine-tuning procedures \cite  {devlin-2019-bert}}}{12}{figure.2.8}\protected@file@percent }
\newlabel{fig:bert_finetuning}{{\M@TitleReference {2.8}{BERT pre-training and fine-tuning procedures \cite  {devlin-2019-bert}}}{12}{BERT pre-training and fine-tuning procedures \cite {devlin-2019-bert}}{figure.2.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}MAGPIE}{12}{section.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Chunking and Hierarchical Techniques}{12}{section.2.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces MAGPIE containing a pre-trained representation of various biases \cite  {horych-2024-magpie}.}}{13}{figure.2.9}\protected@file@percent }
\newlabel{fig:magpie}{{\M@TitleReference {2.9}{MAGPIE containing a pre-trained representation of various biases \cite  {horych-2024-magpie}.}}{13}{MAGPIE containing a pre-trained representation of various biases \cite {horych-2024-magpie}}{figure.2.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Illustration of chunking}}{13}{figure.2.10}\protected@file@percent }
\newlabel{fig:chunking}{{\M@TitleReference {2.10}{Illustration of chunking}}{13}{Illustration of chunking}{figure.2.10}{}}
\citation{wu-2021-hi-transformer}
\citation{wu-2021-hi-transformer}
\citation{borko-1963-auto-doc-classification}
\citation{luhn-1958-business-intelligence-system}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces An example of hierarchical transformer model, Hi-Transformer\cite  {wu-2021-hi-transformer}, to model long document}}{14}{figure.2.11}\protected@file@percent }
\newlabel{fig:hi_transformer}{{\M@TitleReference {2.11}{An example of hierarchical transformer model, Hi-Transformer\cite  {wu-2021-hi-transformer}, to model long document}}{14}{An example of hierarchical transformer model, Hi-Transformer\cite {wu-2021-hi-transformer}, to model long document}{figure.2.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.8}Document Classification}{14}{section.2.8}\protected@file@percent }
\citation{box-2023-untapped}
\citation{mishra-2017-structured-unstructured}
\citation{mali-2021-relevance-of-preprocessing}
\citation{afzal-deepdocclassifier}
\citation{liu-2017-xmlcnn}
\citation{yang-2016-han}
\citation{adhikari-2019-docbert}
\citation{wan-2019-long-length}
\citation{beltagy-2020-longformer}
\citation{zaheer-2021-bigbird}
\citation{park-2022-efficient}
\citation{pappagari-2019-hierarchical}
\citation{su-2021-classifying}
\citation{adhikari-2019-docbert}
\citation{khandve-2022-hierarchical-longdoc}
\@writefile{toc}{\contentsline {section}{\numberline {2.9}Article-Level Media Bias Classification}{15}{section.2.9}\protected@file@percent }
\citation{fan-2019-basil}
\citation{chen-2020-nlpcss}
\citation{spinde-2023-bat}
\citation{lim-2018-understanding}
\citation{spinde-2021-babe}
\citation{spinde-2021-bias-words}
\citation{allsides}
\citation{adfontes}
\citation{spinde-2023-bat}
\citation{chen-2020-nlpcss}
\citation{kulkarni-2018-multi-view}
\citation{spinde-2023-bat}
\citation{devlin-2019-bert}
\citation{liu-2019-roberta}
\citation{maab-2023-lexical-bias-detection}
\citation{maab-2023-target-aware}
\citation{guo-2022-modeling}
\citation{van-den-berg-2020-context}
\citation{lee-2021-unifying}
\citation{lei-2022-sentence}
\citation{lei-2024-event-relation}
\citation{demidov-2023-political-bias-classification}
\citation{chen-2020-detecting-media-bias-gaussian}
\citation{kulkarni-2018-multi-view}
\citation{barbera-2021-article-classification}
\@setckpt{chap-2}{
\setcounter{page}{18}
\setcounter{equation}{4}
\setcounter{enumi}{4}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{@memmarkcntra}{0}
\setcounter{storedpagenumber}{1}
\setcounter{book}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{9}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{vslineno}{0}
\setcounter{poemline}{0}
\setcounter{modulo@vs}{0}
\setcounter{memfvsline}{0}
\setcounter{verse}{0}
\setcounter{chrsinstr}{0}
\setcounter{poem}{0}
\setcounter{newflo@tctr}{4}
\setcounter{@contsubnum}{0}
\setcounter{maxsecnumdepth}{2}
\setcounter{sidefootnote}{0}
\setcounter{pagenote}{0}
\setcounter{pagenoteshadow}{0}
\setcounter{memfbvline}{0}
\setcounter{bvlinectr}{0}
\setcounter{cp@cntr}{0}
\setcounter{ism@mctr}{0}
\setcounter{xsm@mctr}{0}
\setcounter{csm@mctr}{0}
\setcounter{ksm@mctr}{0}
\setcounter{xksm@mctr}{0}
\setcounter{cksm@mctr}{0}
\setcounter{msm@mctr}{0}
\setcounter{xmsm@mctr}{0}
\setcounter{cmsm@mctr}{0}
\setcounter{bsm@mctr}{0}
\setcounter{workm@mctr}{0}
\setcounter{sheetsequence}{22}
\setcounter{lastsheet}{61}
\setcounter{lastpage}{57}
\setcounter{figure}{11}
\setcounter{lofdepth}{1}
\setcounter{table}{0}
\setcounter{lotdepth}{1}
\setcounter{section@level}{1}
\setcounter{Item}{4}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{15}
\setcounter{memhycontfloat}{0}
\setcounter{Hpagenote}{0}
\setcounter{parentequation}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{0}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
}
