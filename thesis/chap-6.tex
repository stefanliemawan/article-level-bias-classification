\chapter{Evaluation}
\label{cha:6}

All methods are implemented using the PyTorch \cite{paszke-2017-pytorch} and transformer \cite{wolf-2020-huggingface} package from HuggingFace. The batch size is set to 8, with epochs ranging between 4-6 for standard fine-tuning. For every method, precision, recall, and F1 score is evaluated on both overall and per class performance. It is particularly important to assess how well the model classify biased articles.

\section{Baseline methods}

\begin{table}[htbp]
    \centering
    \begin{tabular}{|| c | c | c | c | c ||}
        \hline
        \multicolumn{5}{|| c ||}{\textbf{Features: title + content}}          \\
        \hline
        Class              & Precision & Recall & F1     & Support            \\
        \hline
        Problematic        & 0.44      & 0.33   & 0.38   & 24                 \\
        \hline
        Questionable       & 0.32      & 0.31   & 0.32   & 51                 \\
        \hline
        Generally Reliable & 0.38      & 0.43   & 0.41   & 99                 \\
        \hline
        Reliable           & 0.86      & 0.85   & 0.85   & 370                \\
        \hline
        Overall            & 0.7043    & 0.6985 & 0.7007 &                    \\
        \hline
        \hline
        \hline
        \hline
        \multicolumn{5}{|| c ||}{\textbf{Features: outlet + title + content}} \\
        \hline
        Class              & Precision & Recall & F1     & Support            \\
        \hline
        Problematic        & 0.45      & 0.38   & 0.41   & 24                 \\
        \hline
        Questionable       & 0.34      & 0.35   & 0.35   & 51                 \\
        \hline
        Generally Reliable & 0.37      & 0.39   & 0.38   & 99                 \\
        \hline
        Reliable           & 0.86      & 0.85   & 0.86   & 370                \\
        \hline
        Overall            & 0.7056    & 0.7003 & 0.7027 &                    \\
        \hline
    \end{tabular}
    \caption{BoW + logistic regression evaluation}
    \label{table:bow-logistic-eval}
\end{table}

\begin{table}[htbp]
    \centering
    \begin{tabular}{|| c | c | c | c | c ||}
        \hline
        \multicolumn{5}{|| c ||}{\textbf{Features: title + content}}          \\
        \hline
        Class              & Precision & Recall & F1     & Support            \\
        \hline
        Problematic        & 1.00      & 0.04   & 0.08   & 24                 \\
        \hline
        Questionable       & 0.56      & 0.29   & 0.38   & 51                 \\
        \hline
        Generally Reliable & 0.44      & 0.41   & 0.42   & 99                 \\
        \hline
        Reliable           & 0.82      & 0.93   & 0.87   & 370                \\
        \hline
        Overall            & 0.7316    & 0.7389 & 0.7094 &                    \\
        \hline
        \hline
        \hline
        \hline
        \multicolumn{5}{|| c ||}{\textbf{Features: outlet + title + content}} \\
        \hline
        Class              & Precision & Recall & F1     & Support            \\
        \hline
        Problematic        & 1.00      & 0.04   & 0.08   & 24                 \\
        \hline
        Questionable       & 0.54      & 0.27   & 0.36   & 51                 \\
        \hline
        Generally Reliable & 0.43      & 0.44   & 0.44   & 99                 \\
        \hline
        Reliable           & 0.82      & 0.92   & 0.87   & 370                \\
        \hline
        Overall            & 0.7336    & 0.7371 & 0.7099 &                    \\
        \hline
    \end{tabular}
    \caption{TF-IDF + logistic regression evaluation}
    \label{table:tfidf_logistic-eval}
\end{table}


In both Table \ref{table:bow-logistic-eval} and Table \ref{table:tfidf_logistic-eval}, it can be seen that frequency-based approaches such as BoW and TF-IDF perform generally well. Including outlet information as features only slighty improve the performance. However, when we look at per-class metrics, it can be seen that the overall scores are heavily influenced by the performance of class 'Reliable' due to its large support. Evidently, the model suffers when classifying underrepresented classes. Furthermore, TF-IDF model seems to perform significantly worse in the 'Problematic' class.

\begin{table}[htbp]
    \centering
    \begin{tabular}{|| c | c | c | c | c ||}
        \hline
        \multicolumn{5}{|| c ||}{\textbf{Features: title + content}}          \\
        \hline
        Class              & Precision & Recall & F1     & Support            \\
        \hline
        \hline
        Problematic        & 0.60      & 0.25   & 0.35   & 24                 \\
        \hline
        Questionable       & 0.47      & 0.53   & 0.50   & 51                 \\
        \hline
        Generally Reliable & 0.37      & 0.56   & 0.45   & 99                 \\
        \hline
        Reliable           & 0.89      & 0.79   & 0.84   & 370                \\
        \hline
        Overall            & 0.7428    & 0.7003 & 0.7132 &                    \\
        \hline
        \hline
        \hline
        \hline
        \multicolumn{5}{|| c ||}{\textbf{Features: outlet + title + content}} \\
        \hline
        Class              & Precision & Recall & F1     & Support            \\
        \hline
        \hline
        Problematic        & 0.67      & 0.42   & 0.51   & 24                 \\
        \hline
        Questionable       & 0.46      & 0.61   & 0.52   & 51                 \\
        \hline
        Generally Reliable & 0.46      & 0.62   & 0.52   & 99                 \\
        \hline
        Reliable           & 0.95      & 0.84   & 0.89   & 370                \\
        \hline
        Overall            & 0.7997    & 0.7535 & 0.7717 &                    \\
        \hline
    \end{tabular}
    \caption{BERT fine-tuning evaluation}
    \label{table:bert-fine-tuning-eval}
\end{table}

Fine-tuning BERT after 6 epochs only performed slightly better than BoW method as can be seen in Table \ref{table:bert-fine-tuning-eval} and Table \ref{table:bow-logistic-eval}. In this method 'bert-base-cased' is used instead of 'bert-base-uncased' to reserve differences in capitalised words, which can be crucial. Moreover, incorporating outlet information as features moderately increase the performance in all accounts.

\begin{table}[htbp]
    \centering
    \begin{tabular}{|| c | c | c | c | c ||}
        \hline
        Class              & Precision & Recall & F1     & Support \\
        \hline
        \hline
        Problematic        & 0.57      & 0.71   & 0.63   & 24      \\
        \hline
        Questionable       & 0.60      & 0.47   & 0.53   & 51      \\
        \hline
        Generally Reliable & 0.56      & 0.55   & 0.55   & 99      \\
        \hline
        Reliable           & 0.91      & 0.93   & 0.92   & 370     \\
        \hline
        Overall            & 0.8007    & 0.8051 & 0.8017 &         \\
        \hline
    \end{tabular}
    \caption{Outlet-based majority votes evaluation}
    \label{table:majority-votes-eval}
\end{table}



As a comparison, using solely outlet information without any textual information (with majority votes) outperformed all baseline methods both overall and in every class. This result signifies how influential outlet information can be used to classify media bias.

\begin{comment}
Find out why TF-IDF performs worse on class problematic
\end{comment}


\section{Sliding window}

For this approach, a window size of 512 (maximum token of BERT) is chosen with a stride of 256. Additionally, only the first 3 chunks of each input sequence will be used, each input sequence will be truncated to only the first 3 chunks, as using longer chunks does not consistently improve the performance, along with increased computation cost.

Evaluation pending.

\section{CLS Method}

For the CLS method, two different language models are used: BERT and MAGPIE, to encode the input in a higher dimensional space. Both approaches with the CLS method are evaluated. A chunk size of 512 is used, 2 transformer layers, 0.2 dropout probability, and 3 epochs.

Evaluation pending.


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
