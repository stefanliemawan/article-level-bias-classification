\chapter{Evaluation}
\label{cha:6}

All methods are implemented using the PyTorch \cite{paszke-2017-pytorch} and transformer \cite{wolf-2020-huggingface} package from HuggingFace. The batch size is set to 8, with epochs ranging between 4-6 for standard finetuning. Every method besides the majority votes only includes textual content as the input sequence (title + content).

\section{Baseline methods}

\begin{table}[htbp]
    \centering
    \begin{tabular}{|| c | c | c | c | c ||}
        \hline
        Class              & Precision & Recall & F1   & Support \\
        \hline
        \hline
        Problematic        & 0.41      & 0.29   & 0.34 & 24      \\
        \hline
        Questionable       & 0.35      & 0.41   & 0.38 & 51      \\
        \hline
        Generally Reliable & 0.41      & 0.42   & 0.42 & 99      \\
        \hline
        Reliable           & 0.87      & 0.85   & 0.86 & 370     \\
        \hline
        Overall            & 0.71      & 0.70   & 0.71 &         \\
        \hline
    \end{tabular}
    \caption{BoW + logistic regression evaluation}
    \label{table:bow_logistic-eval}
\end{table}

\begin{table}[htbp]
    \centering
    \begin{tabular}{|| c | c | c | c | c ||}
        \hline
        Class              & Precision & Recall & F1     & Support \\
        \hline
        \hline
        Problematic        & 1.00      & 0.04   & 0.08   & 24      \\
        \hline
        Questionable       & 0.54      & 0.27   & 0.36   & 51      \\
        \hline
        Generally Reliable & 0.43      & 0.41   & 0.42   & 99      \\
        \hline
        Reliable           & 0.82      & 0.93   & 0.87   & 370     \\
        \hline
        Overall            & 0.7291    & 0.7371 & 0.7070 &         \\
        \hline
    \end{tabular}
    \caption{TF-IDF + logistic regression evaluation}
    \label{table:tfidf_logistic-eval}
\end{table}


\begin{table}[htbp]
    \centering
    \begin{tabular}{|| c | c | c | c | c ||}
        \hline
        Class              & Precision & Recall & F1     & Support \\
        \hline
        \hline
        Problematic        & 0.60      & 0.25   & 0.35   & 24      \\
        \hline
        Questionable       & 0.47      & 0.53   & 0.50   & 51      \\
        \hline
        Generally Reliable & 0.37      & 0.56   & 0.45   & 99      \\
        \hline
        Reliable           & 0.89      & 0.79   & 0.84   & 370     \\
        \hline
        Overall            & 0.7428    & 0.7003 & 0.7132 &         \\
        \hline
    \end{tabular}
    \caption{BERT finetuning evaluation}
    \label{table:bert-finetuning-eval}
\end{table}


\begin{table}[htbp]
    \centering
    \begin{tabular}{|| c | c | c | c | c ||}
        \hline
        Class              & Precision & Recall & F1     & Support \\
        \hline
        \hline
        Problematic        & 0.57      & 0.71   & 0.63   & 24      \\
        \hline
        Questionable       & 0.60      & 0.47   & 0.53   & 51      \\
        \hline
        Generally Reliable & 0.56      & 0.55   & 0.55   & 99      \\
        \hline
        Reliable           & 0.91      & 0.93   & 0.92   & 370     \\
        \hline
        Overall            & 0.8007    & 0.8051 & 0.8017 &         \\
        \hline
    \end{tabular}
    \caption{Outlet-based majority votes evaluation}
    \label{table:majority-votes-eval}
\end{table}


In both Table \ref{table:bow_logistic-eval} and Table \ref{table:tfidf_logistic-eval}, it can be seen that frequency-based approaches such as BoW and TF-IDF already perform quite well if we look at the average metrics. However, when we look at per-class metrics, it can be seen that the overall scores are heavily influenced by the performance of class 'Reliable' due to its large support. It is quite evident that the model suffers when classifying underrepresented classes, this is expected as the dataset is highly-imbalanced. Furthermore, TF-IDF model seems to perform significantly worse in the 'Problematic' class.

Finetuning BERT after 6 epochs only performed slightly better than BoW method as can be seen in Table \ref{table:bert-finetuning-eval} and Table \ref{table:bow_logistic-eval}. In this method 'bert-base-cased' is used instead of 'bert-base-uncased' to reserve differences in capitalised entities.

As a comparison, using solely outlet information without any textual information (with majority votes) outperformed all baseline methods both overall and in every class. This result signifies how influential outlet information can be used to classify media bias.

\begin{comment}
Find out why TF-IDF performs worse on class problematic
\end{comment}


\section{Sliding window}

For this approach, a window size of 512 (maximum token of BERT) is chosen with a stride of 256. Additionally, only the first 3 chunks of each input sequence will be used, each input sequence will be truncated to only the first 3 chunks, as using longer chunks does not consistently improve the performance, along with increased computation cost.

Evaluation pending.

\section{CLS Method}

For the CLS method, two different language models are used: BERT and MAGPIE, to encode the input in a higher dimensional space. Both approaches with the CLS method are evaluated. A chunk size of 512 is used, 2 transformer layers, 0.2 dropout probability, and 3 epochs.

Evaluation pending.


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
