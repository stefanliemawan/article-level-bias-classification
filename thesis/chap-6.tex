\chapter{Conclusion}
\label{cha:6}

The result of this study shows that frequency-based methods such as Bag-of-Words (BoW) and TF-IDF combined with logistic regression have demonstrated decent overall performance. Transformer-based approaches and hierarchical models, while expected to perform much better, have only shown slight to moderate improvements. Nevertheless, hierarchical models with MAGPIE encoder and mean-pooling with 156 chunk size perform the best. The performance difference between using BERT and MAGPIE as encoders for a feature-based approach highlights that models specifically trained for media bias tasks outperform general large language models (LLMs). Fine-tuning Longformer with the first 4096 tokens also yields comparable results and should therefore be used if a more straightforward approach is preferred.

Furthermore, the experiments revealed that mean-pooling performs better with smaller chunk sizes, while CLS-pooling is more effective with larger chunk sizes. Regardless, mean-pooling with smaller chunk sizes still outperforms CLS-pooling across all chunk sizes. This finding is somewhat consistent with other research showing mean-pooling superiority over CLS-pooling \cite{rodrigo-2024-systematic-review-media-bias}.

Incorporating outlet information leads to performance improvement on almost all fine-tuning methods. Although the outlet majority baseline achieves the highest recall and F1 score, relying solely on outlet information as a classifier is not an optimal solution. This method disregards the articles content and stereotypes outlets based on their perceived bias. Additionally, over-time it may become problematic as outlets may change and evolve.

There are a couple of limitations with this project. The BAT dataset used is completely curated through Ad Fontes Media \cite{adfontes}, with article selection and annotations performed by their team. This introduces an inherent bias in the dataset, which subsequently affects trained models. Additionally, since no other work has used the BAT dataset, comparing results with other studies is practically impossible. These factors should be considered when interpreting the results.

% The low performance gains from transformer models may be partly attributed to the presence of noise in article contents, as they disrupt the context from being effectively used. Frequency-based methods are not be affected as much since they only count words and neglect the context.

% It can be hypothesised that addressing these issues by cleaning the data might lead to significant performance improvements for context-reliant approaches, allowing them to moderately outperform conventional methods. 


\section{Future works}

The BAT dataset requires more samples, particularly for the underrepresented classes, and necessitates further and thorough text cleaning of the crawled article contents. Future work should also aim to include more diverse and independently curated articles to mitigate underlying biases within the dataset. Crowdsourcing \cite{spinde-newsunravel} and using LLMs to augment and annotate media bias datasets \cite{horych-2024-promises} are both promising approaches to solve the dataset problem.

Additionally, evaluation can be improved by carefully crafting the test set by considering unique patterns and including articles with outlets that are not present in the training set. This is crucial for assessing the models' generalisability and ensuring their robustness in real-world scenarios.

Hierarchical methods have shown promising results and should be explored further for improved performance. Additionally, graph-based methods could be considered, as Aires et al. used information theory and community detection algorithms to identify political orientations \cite{aires-2020-information}. A similar approach could be applied to article-level media bias classification by encoding articles as nodes within a graph. This method would allow for more granular analysis and help define relationships between articles.

Other metadata from articles, such as their publication dates, authors, and additional insights like sentiment analysis or Named Entity Recognition (NER), can be utilised to improve performance. However, it is arguably crucial that metadata should remain supplemental, while the primary basis for the classifier should continue to be the articles titles and content.

% Further pre-train BERT in the new domain...


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
