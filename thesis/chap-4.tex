\chapter{Methodology}
\label{cha:4}

This chapter details the preparation of the dataset for training, including the selection of features and establishment of baselines. It also provides a comprehensive discussion of the proposed methods.


\section{Dataset split}

The dataset reliability scores are divided into 4 classes, in alignment with the evaluation criteria set forth by Ad Fontes \cite{adfontes-bias-reliability}, as also previously described in Section \ref{bat-characteristics}. Scores below 24 will be grouped as 'Problematic' while scores above 40 will be regarded as 'Reliable'. As scores between 24 and 40 is described to suggest a variety of factor, this range is equally split into half, the lower-half (24-32) as 'Questionable', and the higher-half with (31-40) as 'Generally Reliable'. The full class split is shown in Table \ref{table:label_split}.

\begin{table}[htbp]
    \centering
    \begin{tabular}{| l | l |}
        \hline
        Class              & Range         \\
        \hline
        Problematic        & 0.00 - 24.00  \\
        Questionable       & 24.01 - 32.00 \\
        Generally Reliable & 32.01 - 40.00 \\
        Reliable           & 40.01 - 64    \\
        \hline
    \end{tabular}
    \caption{Reliability scores are divided into four classes based on their values: Problematic, Questionable, Generally Reliable, and Reliable.}
    \label{table:label_split}
\end{table}

As can be seen in Table \ref{table:dataset_split}, the dataset is highly imbalanced with over half of the articles rated within the 'Reliable' class. The class percentage is as follows: Problematic: 6.34\%, Questionable: 13.37\%, Generally Reliable: 23.01\%, Reliable: 57.28\%.

The dataset is split into train, test, and validation sets, distributed as seen in Table \ref{table:dataset_split}. The split is done in a manner that ensure that articles from different outlets are distributed equally across the three sets. A significant drawback of this balanced approach is that neither the test set nor the validation set contains articles from unseen outlets. This limitation could potentially introduce bias in the performance evaluation and test metrics, as models trained on this dataset are not exposed to articles from outlets not seen in the training set. Ideally, a test set should contain information not included in the training set to better assess model performance. However, given the imbalance within the dataset, it is preferable to make full use of the available data by exposing models to as many patterns as possible. Additionally, since new outlets are rarely introduced in real life, it may instead be advantageous to slightly overfit models to patterns of existing outlets.

\begin{table}[htbp]
    \centering
    \begin{tabular}{| l | c | c | c | l |}
        \hline
        Class              & Train Set & Test Set & Validation Set & Total           \\
        \hline
        Problematic        & 287       & 27       & 34             & 348   (6.34\%)  \\
        Questionable       & 611       & 54       & 70             & 735   (13.37\%) \\
        Generally Reliable & 1033      & 104      & 128            & 1265  (23.01\%) \\
        Reliable           & 2394      & 384      & 371            & 3149  (57.28\%) \\
        \hline
        Total              & 4325      & 569      & 603            & 5497            \\
        \hline
    \end{tabular}
    \caption{Number of total samples in the dataset and number of samples for each class in train, test, and validation set}
    \label{table:dataset_split}
\end{table}


\section{Features and Baselines}

The primary features are the titles and contents of the articles, each concatenated into a single sequence, which is then tokenised.  Outlet information is incorporated as an additional feature in certain cases, to enable a comparative analysis of classifiers' performance with and without the inclusion of this supplementary data.

As baselines, traditional frequency-based encoding methods such as Bag-of-Words and TF-IDF are implemented, each combined with a simple logistic regression classifier. In these methods, the validation set is merged with the training set, as no validation step is performed.

Additionally, an \textbf{outlet majority baseline} is implemented as a comparison to analyse the influence of including outlet information as an additional feature. This baseline method predicts the class of an article based on the most frequently occurring class among articles from the same outlet. Specifically, if an article comes from outlet A, and the majority of articles from outlet A are classified as class X, then the article from outlet A is also classified as class X. An illustration can be seen in Figure \ref{fig:majority_baseline}.

For baselines and frequency-based approaches (BoW and TF-IDF), tokenisation is done on the word-level, whereas transformer-approaches will utilise sub-word tokenisation as implemented in BERT \cite{devlin-2019-bert}. No additional text preprocessing is performed for any of the methods.


\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}
        \tikzstyle{outlet} = [circle, draw, fill=brown!20, text centered]
        \tikzstyle{prediction} = [rectangle, draw, fill=red!20, text centered]
        \tikzstyle{article} = [circle, draw, fill=gray!20, text centered, font=\footnotesize]
        \tikzstyle{arrow} = [thick,->,>=stealth]
        \tikzstyle{label} = [text centered, font=\footnotesize]

        \node[outlet] (outletA) at (0, 6) {Outlet A};

        \node[article] (A1) at (3, 12) {Article 1};
        \node[article] (A2) at (3, 9) {Article 2};
        \node[article] (A3) at (3, 6) {Article 3};
        \node[article] (A4) at (3, 3) {Article 4};
        \node[article] (A5) at (3, 0) {Article 5};

        \draw[arrow] (outletA) -- (A1);
        \draw[arrow] (outletA) -- (A2);
        \draw[arrow] (outletA) -- (A3);
        \draw[arrow] (outletA) -- (A4);
        \draw[arrow] (outletA) -- (A5);

        \node[prediction] (ClassA1) at (8, 12) {Predicted class: X};
        \node[prediction] (ClassA2) at (8, 9) {Predicted class: X};
        \node[prediction] (ClassA3) at (8, 6) {Predicted class: X};
        \node[prediction] (ClassA4) at (8, 3) {Predicted class: X};
        \node[prediction] (ClassA5) at (8, 0) {Predicted class: X};

        \draw[arrow] (A1) -- (ClassA1);
        \draw[arrow] (A2) -- (ClassA2);
        \draw[arrow] (A3) -- (ClassA3);
        \draw[arrow] (A4) -- (ClassA4);
        \draw[arrow] (A5) -- (ClassA5);

        \node[label] at (4, 11) {Article 1 has class X};
        \node[label] at (4, 8) {Article 2 has class Y};
        \node[label] at (4, 5) {Article 3 has class X};
        \node[label] at (4, 2) {Article 4 has class Z};
        \node[label] at (4, -1) {Article 5 has class X};
    \end{tikzpicture}
    \caption{Majority baseline based on outlet, every article here will be predicted as X since X is the most frequently observed class in outlet A}
    \label{fig:majority_baseline}
\end{figure}

\section{Proposed methods}

In this section, various methods to classify media bias at the article-level are described. These methods are based on three main concepts: (1) frequency-based techniques, (2) transformer-based approaches for handling longer sequences, and (3) methods from past works that have been proven successful in document classification and media bias detection.


\subsection{BoW + MLP}

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}
        \tikzstyle{arrow} = [thick,->,>=stealth]

        \node (input) [draw, align=center, fill=brown!20] {Bag-of-words input};
        \node (dense) [draw, below of=input, fill=green!20] {Dense};
        \draw[arrow] (input.south) -- (dense.north);
        \node (dropout) [draw, right of=dense, xshift=2cm, fill=blue!20] {Dropout};
        \draw[arrow] (dense.east) -- node[above, midway, fill=white] {ReLU} (dropout.west);
        \node (softmax) [draw, right of=dropout, xshift=2cm, fill=cyan!20] {Softmax};
        \draw[arrow] (dropout.east) -- (softmax.west);
        \node (pred) [draw, right of=softmax, xshift=2cm, fill=red!20] {Predicted class};
        \draw[arrow] (softmax.east) -- (pred.west);

    \end{tikzpicture}
    \caption{BoW + MLP architecture. The input article is encoded as a bag of words, then feed into a single linear multilayer perceptron before softmax operation.}
    \label{fig:bow_mlp_architecture}
\end{figure}

As the name suggests, the BoW + MLP method combines Bag-of-Words encoding method with a neural network. Figure \ref{fig:bow_mlp_architecture} shows the simple diagram of this method. First, the text input is encoded using the Bag-of-Words (BoW) method, resulting in a vector representing the frequency of each word. This vector is then passed through a dense layer with a ReLU activation function, followed by a dropout layer. Finally, the output is processed through a softmax function to produce the prediction probabilities. The network is trained for 10 epochs with a learning rate of 2e-5. The size for the dense layer is set to 128 with a dropout probability of 0.2, no warm-up steps are applied.

\subsection{Fine-tuning}

A standard fine-tuning operation is performed as described in the original BERT paper \cite{devlin-2019-bert}. Input texts are tokenised on the sub-words level and fed into a pre-trained (BERT-like) model, which are then fine-tuned for a specific downstream task. Three pre-trained models are evaluated in this method: BERT \cite{devlin-2019-bert}, Longformer \cite{beltagy-2020-longformer}, and BigBird \cite{zaheer-2021-bigbird}. For BERT, only the first 512 tokens are fed into the model, due to its token limitations, while both Longformer and BigBird allows for up to 4096 tokens. Training is performed over 4 epochs with 2e-5 learning rate and 500 linear warm-up steps.

\subsection{Chunked fine-tuning}

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}
        \tikzstyle{arrow} = [thick,->,>=stealth]
        \tikzstyle{chunk} = [rectangle, draw, fill=gray!20]
        \tikzstyle{label} = [text centered,font=\footnotesize]

        \node[label] at (0, 1) {Text input};
        \node (input) [draw, align=left, text width=0.9\linewidth, font=\tiny, fill=brown!20] at (0, 0) {Nearly 190,000 people tried to illegally enter the United States from its southern border with Mexico in June – the highest number in more than 20 years, and as much as ten times the total recorded during some months under Trump. According to data from US Customs and Border Protection, June saw 188,829 illegal immigrants attempt to cross the border...};

        \node[chunk] (chunk1) [draw] at (-5, -2) {Chunk 1};
        \node[chunk] (chunk2) [draw] at (0, -2) {Chunk 2};
        \node[label] at (2.25, -2) {.......};
        \node[chunk] (chunk3) [draw] at (5, -2) {Chunk \(n\)};

        \draw[arrow] (input) -- (chunk1);
        \draw[arrow] (input) -- (chunk2);
        \draw[arrow] (input) -- (chunk3);

        \node (bert) [draw, circle, fill=green!20] at (0, -4) {LLM};
        \node[label] at (1.75, -4) {fine-tune};

        \draw[arrow] (chunk1) -- (bert);
        \draw[arrow] (chunk2) -- (bert);
        \draw[arrow] (chunk3) -- (bert);

        \node[chunk] (logits1) [draw] at (-5, -6) {Logits 1};
        \node[chunk] (logits2) [draw] at (0, -6) {Logits 2};
        \node[label] at (2.25, -6) {.......};
        \node[chunk] (logits3) [draw] at (5, -6) {Logits \(n\)};

        \draw[arrow] (bert) -- (logits1);
        \draw[arrow] (bert) -- (logits2);
        \draw[arrow] (bert) -- (logits3);

        \node (pooling) [draw, fill=blue!10] at (0, -8) {Pooled logits};

        \draw[arrow] (logits1) -- (pooling);
        \draw[arrow] (logits2) -- (pooling);
        \draw[arrow] (logits3) -- (pooling);

        \node (softmax) [draw, fill=cyan!20] at (0, -9.5) {Softmax};

        \draw[arrow] (pooling) -- (softmax);

        \node (pred) [draw, fill=red!20] at (0, -11) {Predicted class};

        \draw[arrow] (softmax) -- (pred);

    \end{tikzpicture}
    \caption{Chunked fine-tuning. The input article is split into chunks, each chunk is processed by the model as a mini batch, and the resulting logits are pooled before applying softmax operation.}
    \label{fig:chunk_bert_finetuning}
\end{figure}

Figure \ref{fig:chunk_bert_finetuning} illustrates the architecture of the chunked fine-tuning for LLMs. Input texts are tokenised and segmented into separate chunks containing parts of text input. Chunks have a fixed linear size, with the final chunk padded accordingly. A 'CLS' token is inserted at the beginning of each chunk, acting as a summary representation of the entire chunk sequence, and a 'SEP' token is inserted at the end of each chunk. Chunks are then stacked together and fed into the model as mini-batches for the fine-tuning process. The logits (output scores) produced by each chunk are then averaged to produce the final logits of the full sequence. Finally, a softmax operation is applied to the final logits to obtain the class probabilities.

The chunking method bypasses the sequence length limitation of most LLMs such as BERT \cite{devlin-2019-bert}, allowing for full representation and processing of text input without any loss of information. In the implementation, a window size of 512 tokens is chosen with 256 stride. Fine-tuning is conducted over 4 epochs using a weighted loss function, a learning rate of 2e-5, and 500 linear warm-up steps.


\subsection{Hierarchical Transformer}

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}
        \tikzstyle{arrow} = [thick,->,>=stealth]
        \tikzstyle{chunk} = [rectangle, draw, fill=gray!20]
        \tikzstyle{label} = [text centered, font=\footnotesize]

        \node[label] at (0, 1) {Text input};
        \node (input) [draw, align=left, text width=0.9\linewidth, font=\tiny, fill=brown!20] at (0, 0) {Nearly 190,000 people tried to illegally enter the United States from its southern border with Mexico in June – the highest number in more than 20 years, and as much as ten times the total recorded during some months under Trump. According to data from US Customs and Border Protection, June saw 188,829 illegal immigrants attempt to cross the border...};

        \node[chunk] (chunk1) [draw] at (-5, -2) {Chunk 1};
        \node[chunk] (chunk2) [draw] at (0, -2) {Chunk 2};
        \node[label] at (2.25, -2) {.......};
        \node[chunk] (chunk3) [draw] at (5, -2) {Chunk \(n\)};
        \node[label] at (6, -2.5) {size of \(l\)};

        \draw[arrow] (input) -- (chunk1);
        \draw[arrow] (input) -- (chunk2);
        \draw[arrow] (input) -- (chunk3);

        \node (encoder) [draw, circle, fill=green!20] at (0, -4) {LLM encoder};
        \node[label] at (4, -4.75) {extract last hidden state};

        \draw[arrow] (chunk1) -- (encoder);
        \draw[arrow] (chunk2) -- (encoder);
        \draw[arrow] (chunk3) -- (encoder);

        \node[chunk] (embedding1) [draw] at (-5, -6) {Embeddings 1};
        \node[chunk] (embedding2) [draw] at (0, -6) {Embeddings 2};
        \node[label] at (2.25, -6) {.......};
        \node[chunk] (embedding3) [draw] at (5, -6) {Embeddings \(n\)};
        \node[label] at (6, -6.5) {size of \( l \times d \)};

        \draw[arrow] (encoder) -- (embedding1);
        \draw[arrow] (encoder) -- (embedding2);
        \draw[arrow] (encoder) -- (embedding3);

        \node (tf_layer1) [draw, rectangle, fill=green!20] at (0, -8) {Transformer layer 1};

        \draw[arrow] (embedding1) -- (tf_layer1);
        \draw[arrow] (embedding2) -- (tf_layer1);
        \draw[arrow] (embedding3) -- (tf_layer1);

        \node (tf_layer2) [draw, rectangle, fill=green!20] at (0, -9) {Transformer layer 2};
        \draw[arrow] (tf_layer1) -- (tf_layer2);

        \node[label] at (4, -9.75) {pooling operation};

        \node[chunk] (pooled1) [draw] at (-5, -11) {Pooled emb 1};
        \node[chunk] (pooled2) [draw] at (0, -11) {Pooled emb 2};
        \node[label] at (2.25, -11) {.......};
        \node[chunk] (pooled3) [draw] at (5, -11) {Pooled emb \(n\)};
        \node[label] at (6, -11.5) {size of \(d\)};

        \draw[arrow] (tf_layer2) -- (pooled1);
        \draw[arrow] (tf_layer2) -- (pooled2);
        \draw[arrow] (tf_layer2) -- (pooled3);

        \node (mlp) [draw, circle, fill=green!20] at (0, -13.25) {MLP};
        \node[label] at (4, -13.25) {two linear layers with ReLU and dropouts};

        \draw[arrow] (pooled1) -- (mlp);
        \draw[arrow] (pooled2) -- (mlp);
        \draw[arrow] (pooled3) -- (mlp);

        \node (softmax) [draw, fill=cyan!20] at (0, -14.75) {Softmax};

        \draw[arrow] (mlp) -- (softmax);

        \node (pred) [draw, fill=red!20] at (0, -16) {Predicted class};

        \draw[arrow] (softmax) -- (pred);
    \end{tikzpicture}
    \caption{Hierarchical transformer model architecture.}
    \label{fig:hi_model}
\end{figure}


Figure \ref{fig:hi_model} shows the full architecture of the hierarchical transformer model, similarly implemented by Su et al. \cite{su-2021-classifying} for classifying long clinical documents. This method begins by similarly segmenting the input text into smaller chunks. These chunks are encoded into a higher dimensional feature space by feeding them into a pre-trained Large Language Model (LLM) and extracting the last hidden state, also known as \textit{feature-based approach} as described in \cite{sun-2020-fine-tune}. Subsequently, the encoded chunks are then passed into two transformer layers. Afterwards, each chunk is pooled together, resulting in embedding of size \(d\), serving as a concise summary of the entire chunk sequence. This summary representation is then processed through a Multi-Layer Perceptron (MLP). Lastly, a softmax operation will be applied to the output of the MLP layer to determine the final output class.

Given that automated text classification of news articles has been shown to benefit from using segments instead of sentences as units of analysis \cite{barbera-2021-article-classification}, a segment-length window size is utilised instead of sentence-based splitting. Two different pre-trained models are experimented as the LLM encoder in this method: BERT \cite{devlin-2019-bert}, DA-RoBERTa \cite{krieger-2022-domain}, and MAGPIE \cite{horych-2024-magpie}. The MLP consists of two linear layers with ReLU activation function \cite{agarap-2018-relu} and dropouts. Two pooling strategies are used for this method: CLS pooling with chunk size of 512 tokens and mean pooling with smaller chunk size of 156 tokens. Two transformer layers are used, and 0.2 probability is applied to the dropout layer. Training is done over 3 epochs with weighted loss, 1e-5 learning rate and 162 warm-up steps (10\% of total training steps). 


\section{Training details}

All methods are implemented in Python 3.12.0 \cite{van-1995-python} using the PyTorch \cite{paszke-2017-pytorch} and transformers \cite{wolf-2020-huggingface} package from HuggingFace. The BERT model utilised in these methods is the 'bert-base-cased' instead of the 'bert-base-uncased' to account for distinctions in capitalised words \cite{devlin-2019-bert}. The AdamW \cite{loshchilov-2019-adamw} optimiser and Cross Entropy Loss is used in all cases. The batch size is set to 8 for all deep learning methods.

Additionally, \textbf{weighted loss} is applied for every method. Weighted loss addresses the problem of training models on an imbalanced dataset by assigning higher weights to classes with fewer instances and lower weights to classes with more instances. This adjustment ensures that the model pays more attention to correctly predicting the minority class, thereby improving overall performance metrics. Using weighted loss during training is preferred over oversampling the minority class. Oversampling can generate synthetic examples that may not accurately represent real-world articles, potentially leading to less robust model performance \cite{alkhawaldeh-2023-challenges}. Additionally, undersampling is also not ideal due to the small size of the dataset and the significant disparity between the majority and minority classes. The weighted loss is gained by calculating class weights (as seen in Figure \ref{fig:class_weights}) through the scikit-learn library \cite{pedregosa-2011-scikit-learn}, which are then inserted into the loss function.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/class_weights.png}
    \caption{Class weights in the training set}
    \label{fig:class_weights}
\end{figure}



%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
